基于深度确定策略梯度的（DDPG）架构通常用于处理连续动作空间中的移动机器人导航任务。

DDPG是一个基于行动者-评论家的模型，其中行动者网络将运动控制输出为真实标量，评论家评估状态-动作对。

DDPG算法的一个消极方面是其倾向于高估Q值，从而使学习不稳定[17]。

**DRL中的神经网络权重优化通过选择的奖励函数来实现。在引导机器人运动任务中，主要的奖励选择标准相当明确，并且有充分的文档记录。如果达到目标，环境将返回高的正奖励，但如果达到失败状态，环境将获得大的负奖励。然而，仅基于这两个奖励标准的政策培训可能非常困难，特别是如果回报很少的话。神经网络在探索过程中可能不会遇到任何这些状态，因此限制了从这些轨迹中学习的能力。此外，这不包括将运动嵌入任何类型的行为，因为策略只会优化达到目标，而不会优化运动本身。为了纠正这个问题，经常使用即时奖励功能。每一个单独的动作步骤都会被评估，并获得奖励。通过这种即时奖励，不仅可以将政策引导到目标，还可以嵌入运动的期望行为。**

与其他学习方法（如有监督或无监督）相比，RL更适合控制机器人运动，因为该方法不需要数据集。文献[6]–[12]中利用RL进行了许多研究。例如，Matulis和Harvay已经使用作为RL方法的近端策略优化（PPO）实现了铰接机器人的数字孪生[8]。在另一项研究中，移动机器人的MIMO PID控制器已使用基于深度确定性策略梯度（DDPG）的方法进行了调整，该方法是RL方法[12]。在另一项研究中，已经形成了混合和端到端DDPG结构来控制电缆驱动的并联机器人。两种结构控制了机器人，但在研究中，混合学习比端到端结构更快[7]。深度p网络和决斗深度p网络已被开发用于n-DoF机器人的到达任务。然后，这些结构实现了填充手帕和折叠t恤的任务[11]。在另一项研究中，通过使用RL和逆RL算法，开发了蛇形机器人的节能和自适应控制结构[10]。利用RL[9]，机器人在迷宫中导航。